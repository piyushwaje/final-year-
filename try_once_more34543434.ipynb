{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "af3bx72zrSlx"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.cross_decomposition import CCA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "import tensorflow.compat.v1 as tf\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "tf.disable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6AkXaLoJn0q",
        "outputId": "5de95392-6847-40be-8ba6-edc7f4a6094e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "2v03ivFtY6Qp"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def load_datasets_from_folder(folder_path, delimiter=','):\n",
        "    \"\"\"\n",
        "    Load all datasets from a folder with .txt extension and return them as a dictionary.\n",
        "\n",
        "    :param folder_path: The common folder path containing datasets.\n",
        "    :param delimiter: The delimiter used in the .txt files (default is comma).\n",
        "    :return: A dictionary where the keys are the dataset filenames (without .txt) and values are the datasets.\n",
        "    \"\"\"\n",
        "    datasets = {}\n",
        "\n",
        "    # List all files in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        # Check if the file is a TXT file\n",
        "        if filename.endswith('.txt'):\n",
        "            # Construct full file path\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "            # Load the dataset using pandas without header (first row is part of data)\n",
        "            datasets[filename.replace('.txt', '')] = pd.read_csv(file_path, delimiter=delimiter, header=None)\n",
        "\n",
        "    return datasets\n",
        "\n",
        "# Usage Example\n",
        "\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Dataset_final'  # Common path for datasets\n",
        "datasets = load_datasets_from_folder(folder_path, delimiter=',')  # Change delimiter as needed\n",
        "\n",
        "# Access and print each dataset from the dictionary\n",
        "# for dataset_name, dataset in datasets.items():\n",
        "#     print(f\"Dataset: {dataset_name}\")\n",
        "#     print(dataset.head())  # Print first few rows to inspect the dataset\n",
        "#     print(\"\\n\" + \"-\"*50 + \"\\n\")  # Add a separator for better readability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Oym9LnXyfpsa",
        "outputId": "3e130877-50e4-4135-f5bf-3785732a811c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     0  1  2  3  4  5  6\n",
              "0    1  1  1  1  1  1  1\n",
              "1    1  1  1  1  1  2  1\n",
              "2    1  1  1  1  3  2  1\n",
              "3    1  1  1  1  4  1  0\n",
              "4    1  1  1  2  1  1  1\n",
              "..  .. .. .. .. .. .. ..\n",
              "427  1  3  2  1  1  2  0\n",
              "428  2  3  2  2  3  1  0\n",
              "429  3  1  2  2  2  2  1\n",
              "430  3  2  2  1  2  2  1\n",
              "431  3  2  2  1  4  1  0\n",
              "\n",
              "[432 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2178436d-556d-4727-a234-11d7c4d5b95a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>432 rows Ã— 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2178436d-556d-4727-a234-11d7c4d5b95a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2178436d-556d-4727-a234-11d7c4d5b95a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2178436d-556d-4727-a234-11d7c4d5b95a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-edd0c873-fb23-40d4-b940-b495128ec2c3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-edd0c873-fb23-40d4-b940-b495128ec2c3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-edd0c873-fb23-40d4-b940-b495128ec2c3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_ab8ecf04-c16e-4e43-a06a-2250c9e64463\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df1')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ab8ecf04-c16e-4e43-a06a-2250c9e64463 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df1');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df1",
              "summary": "{\n  \"name\": \"df1\",\n  \"rows\": 432,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 4,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 5,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 6,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "df1 = datasets['monk2']\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_agO3Hul-kKf",
        "outputId": "8be00298-ac1e-47a5-e6a8-374a29330c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distinct label counts:\n",
            "6\n",
            "1    228\n",
            "0    204\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "label_counts = df1.iloc[:, -1].value_counts()\n",
        "\n",
        "# Print the distinct label counts\n",
        "print(\"Distinct label counts:\")\n",
        "print(label_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "nEKnfKv5fppN"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to separate features and labels\n",
        "def separate_features_labels(dataset):\n",
        "    if dataset is not None:  # Ensure the dataset is not None\n",
        "        X = dataset.iloc[:, :-1]  # All columns except the last one (features)\n",
        "        y = dataset.iloc[:, -1]   # The last column (label/target)\n",
        "        return X, y\n",
        "    else:\n",
        "        print(\"Dataset is None\")\n",
        "        return None, None\n",
        "\n",
        "# Separate features and labels for each dataset and assign variables with \"_original\"\n",
        "monk1, monk1_labels_original = separate_features_labels(datasets.get('monk1'))\n",
        "monk2, monk2_labels_original = separate_features_labels(datasets.get('monk2'))\n",
        "monk3, monk3_labels_original = separate_features_labels(datasets.get('monk3'))\n",
        "WPBC, WPBC_labels_original = separate_features_labels(datasets.get('WPBC'))\n",
        "Wdbc, Wdbc_labels_original = separate_features_labels(datasets.get('Wdbc'))\n",
        "balance, balance_labels_original = separate_features_labels(datasets.get('balance'))\n",
        "cardio, cardio_labels_original = separate_features_labels(datasets.get('cardio'))\n",
        "glass, glass_labels_original = separate_features_labels(datasets.get('glass'))\n",
        "iris1, iris1_labels_original = separate_features_labels(datasets.get('iris1'))\n",
        "pendigit, pendigit_labels_original = separate_features_labels(datasets.get('pendigit'))\n",
        "pima, pima_labels_original = separate_features_labels(datasets.get('pima'))\n",
        "seeds, seeds_labels_original = separate_features_labels(datasets.get('seeds'))\n",
        "\n",
        "# Handle potential misspelling here\n",
        "vehicle, vehicle_labels_original = separate_features_labels(datasets.get('vehicle'))  # Check the correct spelling\n",
        "wine, wine_labels_original = separate_features_labels(datasets.get('wine'))\n",
        "\n",
        "# Function to plot count of labels\n",
        "# def plot_count_labels(labels, title):\n",
        "#     plt.figure(figsize=(8, 6))\n",
        "#     sns.countplot(x=labels)\n",
        "#     plt.title(title)\n",
        "#     plt.xlabel('Labels')\n",
        "#     plt.ylabel('Count')\n",
        "#     plt.xticks(rotation=45)\n",
        "#     plt.show()\n",
        "\n",
        "# List of dataset names and their corresponding labels\n",
        "dataset_names = [\n",
        "    ('monk1', monk1_labels_original),\n",
        "    ('monk2', monk2_labels_original),\n",
        "    ('monk3', monk3_labels_original),\n",
        "    ('WPBC', WPBC_labels_original),\n",
        "    ('Wdbc', Wdbc_labels_original),\n",
        "    ('balance', balance_labels_original),\n",
        "    ('cardio', cardio_labels_original),\n",
        "    ('glass', glass_labels_original),\n",
        "    ('iris1', iris1_labels_original),\n",
        "    ('pendigit', pendigit_labels_original),\n",
        "    ('pima', pima_labels_original),\n",
        "    ('seeds', seeds_labels_original),\n",
        "    ('vehicle', vehicle_labels_original),\n",
        "    ('wine', wine_labels_original)\n",
        "]\n",
        "\n",
        "# Loop through the datasets and plot count plots for labels\n",
        "# for name, labels in dataset_names:\n",
        "#     if labels is not None:\n",
        "#         plot_count_labels(labels, f'Count of Labels in {name.capitalize()} Dataset')\n",
        "#     else:\n",
        "#         print(f\"No labels found for dataset: {name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "DiogavIW7m4W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "qtCggssw7m2O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "JtH9g0u57mx-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "E-rem1VNfpUy"
      },
      "outputs": [],
      "source": [
        "# ******************************************** These line changes in every dataset*********************************************8\n",
        "# X_original = iris1.to_numpy()\n",
        "# y_true1 = iris1_labels_original.to_numpy()\n",
        "# col = iris1.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "GQR8Qktvd3L3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "9KEF1CjUhyoa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "uJ7GkabEhy04"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "HjfncQDBhy4Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "eBOkwY2Qd3PO"
      },
      "outputs": [],
      "source": [
        "# def dataset(dataset,dataset_labels):\n",
        "#     import numpy as np\n",
        "\n",
        "#     # ****************************************Load Iris Dataset Changes ******************************************\n",
        "#     X_original = dataset.to_numpy()\n",
        "#     def normalize(data):\n",
        "#         for i in range(len(data[0])):\n",
        "#             x_max = data[:,i].max()\n",
        "#             x_min = data[:,i].min()\n",
        "#             difference = x_max - x_min\n",
        "#             if difference == 0:\n",
        "#                 data[:, i] = 0\n",
        "#             else:\n",
        "#                 data[:, i] = (data[:, i] - x_min) / difference\n",
        "#         return data\n",
        "\n",
        "#     normalize(X_original)\n",
        "#     y_true1 =dataset_labels.to_numpy()\n",
        "#     col =dataset.columns\n",
        "#     # **************************************************************\n",
        "\n",
        "\n",
        "#     random_indices = np.random.choice(int(X_original.shape[0]),\n",
        "#                                       size=int(X_original.shape[0]/2),\n",
        "#                                       replace=False)\n",
        "#     #X_original1 = X_original.sample(frac=0.5)\n",
        "#     X_original1 = X_original[random_indices, :]\n",
        "\n",
        "\n",
        "#     indices = np.arange(start=0, stop=X_original.shape[0], step=1)\n",
        "\n",
        "\n",
        "#     def disjoint(e,f):\n",
        "#         return list(set(e).symmetric_difference(set(f)))\n",
        "#     indices1 = disjoint(random_indices,indices)\n",
        "#     print(len(indices1))\n",
        "#     X_original2 = X_original[indices1, :]\n",
        "\n",
        "\n",
        "#     df = pd.DataFrame(X_original1, columns=col)\n",
        "\n",
        "#     df_original = pd.DataFrame(X_original2, columns=col)\n",
        "\n",
        "#     np.random.seed(42)\n",
        "#     mask = np.random.rand(*X_original2.shape) < 0.25\n",
        "#     #df_original1 = df_original.sample(frac=0.5)\n",
        "#     df_missing = df_original.mask(mask)\n",
        "\n",
        "\n",
        "#     ##########################################################################################\n",
        "#     mean = SimpleImputer(strategy='mean')\n",
        "#     view1 = pd.DataFrame(mean.fit_transform(df_missing), columns=df.columns)\n",
        "#     median = SimpleImputer(strategy='median')\n",
        "#     view2 = pd.DataFrame(median.fit_transform(df_missing), columns=df.columns)\n",
        "#     mode = SimpleImputer(strategy='most_frequent')\n",
        "#     view3 = pd.DataFrame(mode.fit_transform(df_missing), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=1)\n",
        "#     view4 = pd.DataFrame(knn.fit_transform(df_missing), columns=df.columns)\n",
        "\n",
        "#     knn = KNNImputer(n_neighbors=3)\n",
        "#     view5 = pd.DataFrame(knn.fit_transform(df_missing), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=5)\n",
        "#     view6 = pd.DataFrame(knn.fit_transform(df_missing), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=7)\n",
        "#     view7 = pd.DataFrame(knn.fit_transform(df_missing), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=9)\n",
        "#     view8 = pd.DataFrame(knn.fit_transform(df_missing), columns=df.columns)\n",
        "\n",
        "#     knn = KNNImputer(n_neighbors=11)\n",
        "#     view9 = pd.DataFrame(knn.fit_transform(df_missing), columns=df.columns)\n",
        "\n",
        "#     knn = KNNImputer(n_neighbors=13)\n",
        "#     view10 = pd.DataFrame(knn.fit_transform(df_missing), columns=df.columns)\n",
        "\n",
        "#     merged_views = pd.concat([view1, view2, view3, view4,view5, view6, view7, view8, view9, view10], axis=1)\n",
        "#     #merged_views = pd.concat([view1, view2, view3, view4], axis=1)\n",
        "#     print(\"Merged Views Shape:\", merged_views.shape)\n",
        "\n",
        "\n",
        "\n",
        "#     ##########################################################################################\n",
        "#     df = pd.DataFrame(X_original1, columns=col)\n",
        "#     df_original1 = pd.DataFrame(X_original1, columns=col)\n",
        "#     view1_original = pd.DataFrame(mean.fit_transform(df_original1), columns=df.columns)\n",
        "#     view2_original = pd.DataFrame(median.fit_transform(df_original1), columns=df.columns)\n",
        "#     view3_original = pd.DataFrame(mode.fit_transform(df_original1), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=1)\n",
        "#     view4_original = pd.DataFrame(knn.fit_transform(df_original1), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=3)\n",
        "#     view5_original = pd.DataFrame(knn.fit_transform(df_original1), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=5)\n",
        "#     view6_original = pd.DataFrame(knn.fit_transform(df_original1), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=7)\n",
        "#     view7_original = pd.DataFrame(knn.fit_transform(df_original1), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=9)\n",
        "#     view8_original = pd.DataFrame(knn.fit_transform(df_original1), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=11)\n",
        "#     view9_original = pd.DataFrame(knn.fit_transform(df_original1), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=13)\n",
        "#     view10_original = pd.DataFrame(knn.fit_transform(df_original1), columns=df.columns)\n",
        "#     merged_views_original = pd.concat([view1_original, view2_original, view3_original, view4_original,view5_original, view6_original, view7_original, view8_original, view9_original, view10_original], axis=1)\n",
        "#     #merged_views_original = pd.concat([view1_original, view2_original, view3_original, view4_original], axis=1)\n",
        "#     print(\"Merged Views original Shape:\", merged_views_original.shape)  # Should print (150, 16)\n",
        "\n",
        "\n",
        "\n",
        "#     ##########################################################################################\n",
        "#     np.random.seed(42)\n",
        "#     mask = np.random.rand(*X_original1.shape) < 0.2\n",
        "#     #df_original1 = df_original.sample(frac=0.5)\n",
        "#     df_missing_original = df.mask(mask)\n",
        "#     df_missing_original.shape\n",
        "#     view1_original_missing = pd.DataFrame(mean.fit_transform(df_missing_original), columns=df.columns)\n",
        "#     view2_original_missing = pd.DataFrame(median.fit_transform(df_missing_original), columns=df.columns)\n",
        "#     view3_original_missing = pd.DataFrame(mode.fit_transform(df_missing_original), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=1)\n",
        "#     view4_original_missing = pd.DataFrame(knn.fit_transform(df_missing_original), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=3)\n",
        "#     view5_original_missing = pd.DataFrame(knn.fit_transform(df_missing_original), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=5)\n",
        "#     view6_original_missing = pd.DataFrame(knn.fit_transform(df_missing_original), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=7)\n",
        "#     view7_original_missing = pd.DataFrame(knn.fit_transform(df_missing_original), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=9)\n",
        "#     view8_original_missing = pd.DataFrame(knn.fit_transform(df_missing_original), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=11)\n",
        "#     view9_original_missing = pd.DataFrame(knn.fit_transform(df_missing_original), columns=df.columns)\n",
        "#     knn = KNNImputer(n_neighbors=13)\n",
        "#     view10_original_missing = pd.DataFrame(knn.fit_transform(df_missing_original), columns=df.columns)\n",
        "#     merged_views_original_missing = pd.concat([view1_original_missing, view2_original_missing, view3_original_missing, view4_original_missing,view5_original_missing, view6_original_missing, view7_original_missing, view8_original_missing, view9_original_missing, view10_original_missing], axis=1)\n",
        "#     #merged_views_original = pd.concat([view1_original, view2_original, view3_original, view4_original], axis=1)\n",
        "#     print(\"Merged Views original Shape:\", merged_views_original_missing.shape)  # Should print (150, 16)\n",
        "\n",
        "\n",
        "\n",
        "#     ##########################################################################################\n",
        "#     #Autoencoder (Normalization)\n",
        "\n",
        "\n",
        "#     from sklearn.decomposition import PCA\n",
        "#     from sklearn.manifold import TSNE\n",
        "\n",
        "#     def normalize(data):\n",
        "#         for i in range(len(data[0])):\n",
        "#             x_max = data[:,i].max()\n",
        "#             x_min = data[:,i].min()\n",
        "#             difference = x_max - x_min\n",
        "#             if difference == 0:\n",
        "#                 data[:, i] = 0\n",
        "#             else:\n",
        "#                 data[:, i] = (data[:, i] - x_min) / difference\n",
        "#         return data\n",
        "\n",
        "#     #ALL PARAMETERS\n",
        "#     n_input = merged_views.shape[1]\n",
        "#     n_hidden = n_input * 10\n",
        "#     learning_rate = 0.0001\n",
        "#     epochs = 10000\n",
        "\n",
        "#     # Placeholder\n",
        "#     X = tf.placeholder(tf.float32, shape=[None, n_input])\n",
        "#     Y = tf.placeholder(tf.float32, shape=[None, n_input])\n",
        "\n",
        "#     # Weights and biases\n",
        "#     W1 = tf.Variable(tf.random_uniform([n_input, n_hidden], -0.01, 0.01))\n",
        "#     b1 = tf.Variable(tf.zeros([n_hidden]), name=\"Bias1\")\n",
        "\n",
        "#     W2 = tf.Variable(tf.random_uniform([n_hidden, n_input], -0.01, 0.01))\n",
        "#     b2 = tf.Variable(tf.zeros([n_input]), name=\"Bias2\")\n",
        "\n",
        "#     # Model\n",
        "#     L2 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "#     hy = tf.sigmoid(tf.matmul(L2, W2) + b2)\n",
        "#     cost = tf.reduce_mean(tf.pow(tf.subtract(Y, hy), 2))\n",
        "\n",
        "#     # Optimizer\n",
        "#     #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "#     #optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "#     #optimizer = tf.train.MomentumOptimizer(learning_rate).minimize(cost)\n",
        "#     optimizer = tf.train.MomentumOptimizer(learning_rate, 0.1).minimize(cost);\n",
        "\n",
        "#     #merged_views\n",
        "#     merged_views1_missing = merged_views.to_numpy()\n",
        "#     merged_views1 = merged_views_original.to_numpy()\n",
        "#     merged_views1_original_missing = merged_views_original_missing.to_numpy()\n",
        "\n",
        "\n",
        "#     #seed_value_clustering_array = [0,1,11,3,4,42,5,6,7,8];\n",
        "#     #seed_value_clustering_array = [11];\n",
        "#     #seed_value_clustering_array = [0,1,11,3,4];\n",
        "#     import torch.nn.functional as F\n",
        "#     from sklearn.manifold import MDS\n",
        "#     from sklearn.metrics.pairwise import euclidean_distances\n",
        "#     import torch\n",
        "#     import torch.nn as nn\n",
        "#     class Model(nn.Module):\n",
        "#                 def __init__(self, input_features=n_input, hidden_layer1=n_hidden, output_features=n_input):\n",
        "#                     super().__init__()\n",
        "\n",
        "#                     self.fc1 = nn.Linear(input_features,hidden_layer1)\n",
        "#                     self.out = nn.Linear(hidden_layer1, output_features)\n",
        "\n",
        "#                 def forward(self, x):\n",
        "\n",
        "#                     x1 = F.sigmoid(self.fc1(x))\n",
        "#                     #x = F.relu(self.out(x))\n",
        "#                     x=F.sigmoid(self.out(x1))\n",
        "#                     return x,x1\n",
        "\n",
        "#     lamda1 = 0.0000005\n",
        "#     #mds= MDS(n_components=2,random_state=0)\n",
        "#     #X_scaled = torch.FloatTensor(merged_views1)\n",
        "#     X_scaled = torch.FloatTensor(merged_views_original.to_numpy())\n",
        "#     X_scaled_missing = torch.FloatTensor(merged_views1_missing)\n",
        "#     #X_scaled_original_missing =torch.FloatTensor(merged_views1_original_missing)\n",
        "#     X_scaled_original_missing =torch.FloatTensor(pd.concat([merged_views_original,merged_views_original_missing], axis=0).to_numpy())\n",
        "#     X_scaled1 = torch.FloatTensor(pd.concat([view1_original,view1_original], axis=0).to_numpy())\n",
        "#     X_scaled2 = torch.FloatTensor(pd.concat([view2_original,view2_original], axis=0).to_numpy())\n",
        "#     X_scaled3 = torch.FloatTensor(pd.concat([view3_original,view3_original], axis=0).to_numpy())\n",
        "#     X_scaled4 = torch.FloatTensor(pd.concat([view4_original,view4_original], axis=0).to_numpy())\n",
        "#     X_scaled5 = torch.FloatTensor(pd.concat([view5_original,view5_original], axis=0).to_numpy())\n",
        "#     X_scaled6 = torch.FloatTensor(pd.concat([view6_original,view6_original], axis=0).to_numpy())\n",
        "#     X_scaled7 = torch.FloatTensor(pd.concat([view7_original,view7_original], axis=0).to_numpy())\n",
        "#     X_scaled8 = torch.FloatTensor(pd.concat([view8_original,view8_original], axis=0).to_numpy())\n",
        "#     X_scaled9 = torch.FloatTensor(pd.concat([view9_original,view9_original], axis=0).to_numpy())\n",
        "#     X_scaled10 = torch.FloatTensor(pd.concat([view10_original,view10_original], axis=0).to_numpy())\n",
        "#     X_scaled01 = torch.FloatTensor(view1_original.to_numpy())\n",
        "#     X_scaled02 = torch.FloatTensor(view2_original.to_numpy())\n",
        "#     X_scaled03 = torch.FloatTensor(view3_original.to_numpy())\n",
        "#     X_scaled04 = torch.FloatTensor(view4_original.to_numpy())\n",
        "#     X_scaled05 = torch.FloatTensor(view5_original.to_numpy())\n",
        "#     X_scaled06 = torch.FloatTensor(view6_original.to_numpy())\n",
        "#     X_scaled07 = torch.FloatTensor(view7_original.to_numpy())\n",
        "#     X_scaled08 = torch.FloatTensor(view8_original.to_numpy())\n",
        "#     X_scaled09 = torch.FloatTensor(view9_original.to_numpy())\n",
        "#     X_scaled010 = torch.FloatTensor(view10_original.to_numpy())\n",
        "#     criterion2 = nn.MSELoss()\n",
        "\n",
        "\n",
        "#     model1     = Model()\n",
        "#     criterion1 = nn.CrossEntropyLoss()\n",
        "#     criterion2 = nn.MSELoss()\n",
        "#     optimizer1 = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
        "#     for i in range(epochs):\n",
        "\n",
        "#         if i/2 <epochs/2:\n",
        "#           x1_mds1 = euclidean_distances(X_scaled01, X_scaled01)\n",
        "#           x1_mds2 = euclidean_distances(X_scaled02, X_scaled02)\n",
        "#           x1_mds3 = euclidean_distances(X_scaled03, X_scaled03)\n",
        "#           x1_mds4 = euclidean_distances(X_scaled04, X_scaled04)\n",
        "#           x1_mds5 = euclidean_distances(X_scaled05, X_scaled05)\n",
        "#           x1_mds6 = euclidean_distances(X_scaled06, X_scaled06)\n",
        "#           x1_mds7 = euclidean_distances(X_scaled07, X_scaled07)\n",
        "#           x1_mds8 = euclidean_distances(X_scaled08, X_scaled08)\n",
        "#           x1_mds9 = euclidean_distances(X_scaled09, X_scaled09)\n",
        "#           x1_mds10 = euclidean_distances(X_scaled010, X_scaled010)\n",
        "#           y_pred,w_y_pred = model1.forward(X_scaled)\n",
        "#           x2_mds = euclidean_distances(w_y_pred.detach().numpy(),w_y_pred.detach().numpy())\n",
        "#           squared_difference = np.square(x1_mds1 - x2_mds) + np.square(x1_mds2 - x2_mds) + np.square(x1_mds3 - x2_mds) + np.square(x1_mds4 - x2_mds)+ np.square(x1_mds5 - x2_mds)+ np.square(x1_mds6 - x2_mds)+ np.square(x1_mds7 - x2_mds)+ np.square(x1_mds8 - x2_mds)+ np.square(x1_mds9 - x2_mds)+ np.square(x1_mds10 - x2_mds)\n",
        "#           loss1 = sum(sum(squared_difference)) #/10\n",
        "#           loss = criterion2(y_pred, X_scaled) + lamda1*(loss1)\n",
        "#           if i % 1000 == 0:\n",
        "#                 print(f\"Step: {i}, Error: {loss}\")\n",
        "#           optimizer1.zero_grad()\n",
        "#           loss.backward()\n",
        "#           optimizer1.step()\n",
        "#         else:\n",
        "#           x1_mds1 = euclidean_distances(X_scaled1, X_scaled1)\n",
        "#           x1_mds2 = euclidean_distances(X_scaled2, X_scaled2)\n",
        "#           x1_mds3 = euclidean_distances(X_scaled3, X_scaled3)\n",
        "#           x1_mds4 = euclidean_distances(X_scaled4, X_scaled4)\n",
        "#           x1_mds5 = euclidean_distances(X_scaled5, X_scaled5)\n",
        "#           x1_mds6 = euclidean_distances(X_scaled6, X_scaled6)\n",
        "#           x1_mds7 = euclidean_distances(X_scaled7, X_scaled7)\n",
        "#           x1_mds8 = euclidean_distances(X_scaled8, X_scaled8)\n",
        "#           x1_mds9 = euclidean_distances(X_scaled9, X_scaled9)\n",
        "#           x1_mds10 = euclidean_distances(X_scaled10, X_scaled10)\n",
        "#           y_pred,w_y_pred = model1.forward(X_scaled_original_missing)\n",
        "#           x2_mds = euclidean_distances(w_y_pred.detach().numpy(),w_y_pred.detach().numpy())\n",
        "#           squared_difference = np.square(x1_mds1 - x2_mds) + np.square(x1_mds2 - x2_mds) + np.square(x1_mds3 - x2_mds) + np.square(x1_mds4 - x2_mds)+ np.square(x1_mds5 - x2_mds)+ np.square(x1_mds6 - x2_mds)+ np.square(x1_mds7 - x2_mds)+ np.square(x1_mds8 - x2_mds)+ np.square(x1_mds9 - x2_mds)+ np.square(x1_mds10 - x2_mds)\n",
        "#           loss1 = sum(sum(squared_difference)) #/10\n",
        "#           loss = criterion2(y_pred, X_scaled) + lamda1*(loss1)\n",
        "#           if i % 1000 == 0:\n",
        "#                 print(f\"Step: {i}, Error: {loss}\")\n",
        "#           optimizer1.zero_grad()\n",
        "#           loss.backward()\n",
        "#           optimizer1.step()\n",
        "#     with torch.no_grad():\n",
        "#                 for val in X_scaled_missing:\n",
        "#                     y_pred,w_y_pred = model1.forward(X_scaled_missing)\n",
        "#     encoded_features = w_y_pred.detach().numpy()\n",
        "#     encoded_features1 = encoded_features.copy()\n",
        "#     np.savetxt(\"hidden_layer_output.txt\", encoded_features1, delimiter=\",\")\n",
        "#     hidden_layer_output = np.loadtxt(\"hidden_layer_output.txt\", delimiter=\",\")\n",
        "#     print(\"Hidden Layer Output Shape: \", hidden_layer_output.shape)\n",
        "\n",
        "#     print()\n",
        "#     print()\n",
        "#     print(\" *********************************************************************************** \")\n",
        "#     print()\n",
        "#     print()\n",
        "\n",
        "\n",
        "\n",
        "#     # Train the model\n",
        "#     #from numpy import linalg as LA\n",
        "\n",
        "\n",
        "#     with tf.Session() as session:\n",
        "#         session.run(tf.global_variables_initializer())\n",
        "#         for step in range(epochs):\n",
        "#           if step<epochs/2:\n",
        "#             session.run(optimizer, feed_dict={X: merged_views_original, Y: merged_views_original})\n",
        "#             hidden_layer_output = session.run(L2, feed_dict={X: merged_views_original})\n",
        "#             error = session.run(cost, feed_dict={X: merged_views_original, Y: merged_views_original})\n",
        "#           else:\n",
        "#             session.run(optimizer, feed_dict={X: np.concatenate([merged_views_original,merged_views_original_missing],axis=0), Y: np.concatenate([merged_views_original,merged_views_original],axis=0)})\n",
        "#             hidden_layer_output = session.run(L2, feed_dict={X: merged_views_original_missing})\n",
        "#             error = session.run(cost, feed_dict={X: np.concatenate([merged_views_original,merged_views_original_missing],axis=0), Y: np.concatenate([merged_views_original,merged_views_original],axis=0)})\n",
        "#             #loss2 =\n",
        "#           if step % 10000 == 0:\n",
        "#                 #error = session.run(cost, feed_dict={X: merged_views_original_missing, Y: merged_views_original})\n",
        "#                 print(f\"Step: {step}, Error: {error}\")\n",
        "\n",
        "#         reconstruction_error = session.run(cost, feed_dict={X: merged_views_original_missing, Y: merged_views_original})\n",
        "#         print(\"Reconstruction Error on Data: \", reconstruction_error)\n",
        "\n",
        "#         # Get hidden layer output\n",
        "#         hidden_layer_output = session.run(L2, feed_dict={X: merged_views})\n",
        "\n",
        "#         # Save the hidden layer output\n",
        "#         np.savetxt(\"hidden_layer_output.txt\", hidden_layer_output, delimiter=\",\")\n",
        "\n",
        "#     # Reload hidden layer output and print its shape\n",
        "#     hidden_layer_output = np.loadtxt(\"hidden_layer_output.txt\", delimiter=\",\")\n",
        "#     print(\"Hidden Layer Output Shape: \", hidden_layer_output.shape)\n",
        "\n",
        "#     # from numpy import linalg as LA\n",
        "#     # print(LA.norm(view1))\n",
        "\n",
        "#     # K-Means Clustering for Hidden Layer Output\n",
        "#     import numpy as np\n",
        "#     from sklearn.decomposition import PCA\n",
        "#     pca = PCA(n_components=4)\n",
        "#     hidden_layer_output1= np.concatenate((hidden_layer_output,merged_views),axis=1)\n",
        "#     n_clusters = 3\n",
        "#     kmeans_hidden = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "#     #x1_pca = pca.fit_transform(hidden_layer_output)\n",
        "#     kmeans_hidden.fit(hidden_layer_output)\n",
        "#     labels_hidden = kmeans_hidden.labels_\n",
        "#     # cluster_labels = kmeans.labels_\n",
        "#     print(\" hidden label \", labels_hidden)\n",
        "\n",
        "#     # K-Means Clustering for Original Data\n",
        "#     kmeans_original = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "#     kmeans_original.fit(X_original[indices1, :])\n",
        "#     labels_original = kmeans_original.labels_\n",
        "#     print(\" original label \",labels_original)\n",
        "\n",
        "#     # Calculate NMI and ARI for Hidden Layer Output\n",
        "#     y_true = y_true1 [indices1]\n",
        "#     nmi_hidden = normalized_mutual_info_score(y_true, labels_hidden)\n",
        "#     ari_hidden = adjusted_rand_score(y_true, labels_hidden)\n",
        "\n",
        "#     # Calculate NMI and ARI for Original Data\n",
        "\n",
        "#     print(\"Hidden Layer Output from Autoencoder:\")\n",
        "#     print(f\"NMI: {nmi_hidden:.4f}\")\n",
        "#     print(f\"ARI: {ari_hidden:.4f}\")\n",
        "#     nmi_original = normalized_mutual_info_score(y_true, labels_original)\n",
        "#     ari_original = adjusted_rand_score(y_true, labels_original)\n",
        "#     print(f\"\\nOriginal Dataset:\")\n",
        "#     print(f\"NMI: {nmi_original:.4f}\")\n",
        "#     print(f\"ARI: {ari_original:.4f}\")\n",
        "#     # Apply K-Means for each view\n",
        "#     def apply_kmeans(view, k=3, n_init=10):  # Set n_init explicitly to suppress the warning\n",
        "#         kmeans = KMeans(n_clusters=k, random_state=42, n_init=n_init)\n",
        "#         clusters = kmeans.fit_predict(view)\n",
        "#         return clusters\n",
        "\n",
        "#     # Apply to different views\n",
        "#     clusters_view1 = apply_kmeans(view1, k=3)\n",
        "#     clusters_view2 = apply_kmeans(view2, k=3)\n",
        "#     clusters_view3 = apply_kmeans(view3, k=3)\n",
        "#     clusters_view4 = apply_kmeans(view4, k=3)\n",
        "#     clusters_view5 = apply_kmeans(view5, k=3)\n",
        "#     clusters_view6 = apply_kmeans(view6, k=3)\n",
        "#     clusters_view7 = apply_kmeans(view7, k=3)\n",
        "#     clusters_view8 = apply_kmeans(view8, k=3)\n",
        "#     clusters_view9 = apply_kmeans(view9, k=3)\n",
        "#     clusters_view10 = apply_kmeans(view10, k=3)\n",
        "\n",
        "\n",
        "\n",
        "#     # Evaluate Clustering for each view\n",
        "#     def evaluate_clustering(clusters, true_labels):\n",
        "#         nmi = normalized_mutual_info_score(true_labels, clusters)\n",
        "#         ari = adjusted_rand_score(true_labels, clusters)\n",
        "#         return nmi, ari\n",
        "\n",
        "#     nmi_view1, ari_view1 = evaluate_clustering(clusters_view1, y_true)\n",
        "#     nmi_view2, ari_view2 = evaluate_clustering(clusters_view2, y_true)\n",
        "#     nmi_view3, ari_view3 = evaluate_clustering(clusters_view3, y_true)\n",
        "#     nmi_view4, ari_view4 = evaluate_clustering(clusters_view4, y_true)\n",
        "#     nmi_view5, ari_view5 = evaluate_clustering(clusters_view5, y_true)\n",
        "#     nmi_view6, ari_view6 = evaluate_clustering(clusters_view6, y_true)\n",
        "#     nmi_view7, ari_view7 = evaluate_clustering(clusters_view7, y_true)\n",
        "#     nmi_view8, ari_view8 = evaluate_clustering(clusters_view8, y_true)\n",
        "#     nmi_view9, ari_view9 = evaluate_clustering(clusters_view9, y_true)\n",
        "#     nmi_view10, ari_view10 = evaluate_clustering(clusters_view10, y_true)\n",
        "\n",
        "\n",
        "\n",
        "#     print(f'View 1 => NMI={nmi_view1}, ARI={ari_view1}')\n",
        "#     print(f'View 2 => NMI={nmi_view2}, ARI={ari_view2}')\n",
        "#     print(f'View 3 => NMI={nmi_view3}, ARI={ari_view3}')\n",
        "#     print(f'View 4 => NMI={nmi_view4}, ARI={ari_view4}')\n",
        "#     print(f'View 5 => NMI={nmi_view5}, ARI={ari_view5}')\n",
        "#     print(f'View 6 => NMI={nmi_view6}, ARI={ari_view6}')\n",
        "#     print(f'View 7 => NMI={nmi_view7}, ARI={ari_view7}')\n",
        "#     print(f'View 8 => NMI={nmi_view8}, ARI={ari_view8}')\n",
        "#     print(f'View 9 => NMI={nmi_view9}, ARI={ari_view9}')\n",
        "#     print(f'View 10 => NMI={nmi_view10}, ARI={ari_view10}')\n",
        "\n",
        "#     # Calculate NMI and ARI for merged output\n",
        "#     def calculate_nmi_ari(merged_output, true_labels):\n",
        "#         labels_pred = KMeans(n_clusters=3, n_init=10, random_state=42).fit(merged_output).labels_\n",
        "#         nmi = normalized_mutual_info_score(true_labels, labels_pred)\n",
        "#         ari = adjusted_rand_score(true_labels, labels_pred)\n",
        "#         return nmi, ari\n",
        "\n",
        "#     nmi_merged, ari_merged = calculate_nmi_ari(merged_views, y_true)\n",
        "#     print(f\"Merged View: NMI = {nmi_merged:.4f}, ARI = {ari_merged:.4f}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "xdZ50dxYpTNl"
      },
      "outputs": [],
      "source": [
        "def dataset_newall(dataset,dataset_labels):\n",
        "    import numpy as np\n",
        "\n",
        "    # ****************************************Load Iris Dataset Changes ******************************************\n",
        "    X_original = dataset.to_numpy()\n",
        "    def normalize(data):\n",
        "        for i in range(len(data[0])):\n",
        "            x_max = data[:,i].max()\n",
        "            x_min = data[:,i].min()\n",
        "            difference = x_max - x_min\n",
        "            if difference == 0:\n",
        "                data[:, i] = 0\n",
        "            else:\n",
        "                data[:, i] = (data[:, i] - x_min) / difference\n",
        "        return data\n",
        "\n",
        "    normalize(X_original)\n",
        "    y_true1 =dataset_labels.to_numpy()\n",
        "    col =dataset.columns\n",
        "    # **************************************************************\n",
        "\n",
        "\n",
        "    random_indices = np.random.choice(int(X_original.shape[0]),\n",
        "                                      size=int(X_original.shape[0]/2),\n",
        "                                      replace=False)\n",
        "    #X_original1 = X_original.sample(frac=0.5)\n",
        "    X_original1 = X_original[random_indices, :]\n",
        "\n",
        "\n",
        "    indices = np.arange(start=0, stop=X_original.shape[0], step=1)\n",
        "\n",
        "\n",
        "    def disjoint(e,f):\n",
        "        return list(set(e).symmetric_difference(set(f)))\n",
        "    indices1 = disjoint(random_indices,indices)\n",
        "    print(len(indices1))\n",
        "    X_original2 = X_original[indices1, :]\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(X_original1, columns=col)\n",
        "\n",
        "    df_original = pd.DataFrame(X_original2, columns=col)\n",
        "\n",
        "    np.random.seed(42)\n",
        "    mask = np.random.rand(*X_original2.shape) < 0.05\n",
        "    #df_original1 = df_original.sample(frac=0.5)\n",
        "    df_missing = df_original.mask(mask)\n",
        "\n",
        "\n",
        "    ##########################################################################################\n",
        "    mean = SimpleImputer(strategy='mean')\n",
        "    view1 = pd.DataFrame(mean.fit_transform(df_missing), columns=df.columns)\n",
        "    median = SimpleImputer(strategy='median')\n",
        "    view2 = pd.DataFrame(median.fit_transform(df_missing), columns=df.columns)\n",
        "    mode = SimpleImputer(strategy='most_frequent')\n",
        "    view3 = pd.DataFrame(mode.fit_transform(df_missing), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=1)\n",
        "    view4 = pd.DataFrame(knn.fit_transform(df_missing), columns=df.columns)\n",
        "\n",
        "    knn = KNNImputer(n_neighbors=3)\n",
        "    view5 = pd.DataFrame(knn.fit_transform(df_missing), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=5)\n",
        "    view6 = pd.DataFrame(knn.fit_transform(df_missing), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=7)\n",
        "    view7 = pd.DataFrame(knn.fit_transform(df_missing), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=9)\n",
        "    view8 = pd.DataFrame(knn.fit_transform(df_missing), columns=df.columns)\n",
        "\n",
        "    knn = KNNImputer(n_neighbors=11)\n",
        "    view9 = pd.DataFrame(knn.fit_transform(df_missing), columns=df.columns)\n",
        "\n",
        "    knn = KNNImputer(n_neighbors=13)\n",
        "    view10 = pd.DataFrame(knn.fit_transform(df_missing), columns=df.columns)\n",
        "\n",
        "    merged_views = pd.concat([view1, view2, view3, view4,view5, view6, view7, view8, view9, view10], axis=1)\n",
        "    #merged_views = pd.concat([view1, view2, view3, view4], axis=1)\n",
        "    print(\"Merged Views Shape:\", merged_views.shape)\n",
        "\n",
        "\n",
        "\n",
        "    ##########################################################################################\n",
        "    df = pd.DataFrame(X_original1, columns=col)\n",
        "    df_original1 = pd.DataFrame(X_original1, columns=col)\n",
        "    view1_original = pd.DataFrame(mean.fit_transform(df_original1), columns=df.columns)\n",
        "    view2_original = pd.DataFrame(median.fit_transform(df_original1), columns=df.columns)\n",
        "    view3_original = pd.DataFrame(mode.fit_transform(df_original1), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=1)\n",
        "    view4_original = pd.DataFrame(knn.fit_transform(df_original1), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=3)\n",
        "    view5_original = pd.DataFrame(knn.fit_transform(df_original1), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=5)\n",
        "    view6_original = pd.DataFrame(knn.fit_transform(df_original1), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=7)\n",
        "    view7_original = pd.DataFrame(knn.fit_transform(df_original1), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=9)\n",
        "    view8_original = pd.DataFrame(knn.fit_transform(df_original1), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=11)\n",
        "    view9_original = pd.DataFrame(knn.fit_transform(df_original1), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=13)\n",
        "    view10_original = pd.DataFrame(knn.fit_transform(df_original1), columns=df.columns)\n",
        "    merged_views_original = pd.concat([view1_original, view2_original, view3_original, view4_original,view5_original, view6_original, view7_original, view8_original, view9_original, view10_original], axis=1)\n",
        "    #merged_views_original = pd.concat([view1_original, view2_original, view3_original, view4_original], axis=1)\n",
        "    print(\"Merged Views original Shape:\", merged_views_original.shape)  # Should print (150, 16)\n",
        "\n",
        "\n",
        "\n",
        "    ##########################################################################################\n",
        "    np.random.seed(42)\n",
        "    mask = np.random.rand(*X_original1.shape) < 0.05\n",
        "    #df_original1 = df_original.sample(frac=0.5)\n",
        "    df_missing_original = df.mask(mask)\n",
        "    df_missing_original.shape\n",
        "    view1_original_missing = pd.DataFrame(mean.fit_transform(df_missing_original), columns=df.columns)\n",
        "    view2_original_missing = pd.DataFrame(median.fit_transform(df_missing_original), columns=df.columns)\n",
        "    view3_original_missing = pd.DataFrame(mode.fit_transform(df_missing_original), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=1)\n",
        "    view4_original_missing = pd.DataFrame(knn.fit_transform(df_missing_original), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=3)\n",
        "    view5_original_missing = pd.DataFrame(knn.fit_transform(df_missing_original), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=5)\n",
        "    view6_original_missing = pd.DataFrame(knn.fit_transform(df_missing_original), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=7)\n",
        "    view7_original_missing = pd.DataFrame(knn.fit_transform(df_missing_original), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=9)\n",
        "    view8_original_missing = pd.DataFrame(knn.fit_transform(df_missing_original), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=11)\n",
        "    view9_original_missing = pd.DataFrame(knn.fit_transform(df_missing_original), columns=df.columns)\n",
        "    knn = KNNImputer(n_neighbors=13)\n",
        "    view10_original_missing = pd.DataFrame(knn.fit_transform(df_missing_original), columns=df.columns)\n",
        "    merged_views_original_missing = pd.concat([view1_original_missing, view2_original_missing, view3_original_missing, view4_original_missing,view5_original_missing, view6_original_missing, view7_original_missing, view8_original_missing, view9_original_missing, view10_original_missing], axis=1)\n",
        "    #merged_views_original = pd.concat([view1_original, view2_original, view3_original, view4_original], axis=1)\n",
        "    print(\"Merged Views original Shape:\", merged_views_original_missing.shape)  # Should print (150, 16)\n",
        "\n",
        "\n",
        "\n",
        "    ##########################################################################################\n",
        "    #Autoencoder (Normalization)\n",
        "\n",
        "\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.manifold import TSNE\n",
        "\n",
        "    # def normalize(data):\n",
        "    #     for i in range(len(data[0])):\n",
        "    #         x_max = data[:,i].max()\n",
        "    #         x_min = data[:,i].min()\n",
        "    #         difference = x_max - x_min\n",
        "    #         if difference == 0:\n",
        "    #             data[:, i] = 0\n",
        "    #         else:\n",
        "    #             data[:, i] = (data[:, i] - x_min) / difference\n",
        "    #     return data\n",
        "\n",
        "    #ALL PARAMETERS\n",
        "    n_input = merged_views.shape[1]\n",
        "    n_hidden = n_input * 500\n",
        "    learning_rate = 0.01\n",
        "    epochs = 10000\n",
        "\n",
        "    # Placeholder\n",
        "    X = tf.placeholder(tf.float32, shape=[None, n_input])\n",
        "    Y = tf.placeholder(tf.float32, shape=[None, n_input])\n",
        "\n",
        "    # Weights and biases\n",
        "    W1 = tf.Variable(tf.random_uniform([n_input, n_hidden], -0.01, 0.01))\n",
        "    b1 = tf.Variable(tf.zeros([n_hidden]), name=\"Bias1\")\n",
        "\n",
        "    W2 = tf.Variable(tf.random_uniform([n_hidden, n_input], -0.01, 0.01))\n",
        "    b2 = tf.Variable(tf.zeros([n_input]), name=\"Bias2\")\n",
        "\n",
        "\n",
        "\n",
        "    # Model\n",
        "    L2 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "    hy = tf.sigmoid(tf.matmul(L2, W2) + b2)\n",
        "    cost = tf.reduce_mean(tf.pow(tf.subtract(Y, hy), 2))\n",
        "\n",
        "    # Optimizer\n",
        "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "    #optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "    #optimizer = tf.train.MomentumOptimizer(learning_rate).minimize(cost)\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate, 0.1).minimize(cost);\n",
        "\n",
        "    #merged_views\n",
        "    merged_views1_missing = merged_views.to_numpy()\n",
        "    merged_views1 = merged_views_original.to_numpy()\n",
        "    merged_views1_original_missing = merged_views_original_missing.to_numpy()\n",
        "\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    #from numpy import linalg as LA\n",
        "\n",
        "\n",
        "    with tf.Session() as session:\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        for step in range(epochs):\n",
        "          if step<epochs/2:\n",
        "            session.run(optimizer, feed_dict={X: merged_views_original, Y: merged_views_original})\n",
        "            hidden_layer_output = session.run(L2, feed_dict={X: merged_views_original})\n",
        "            error = session.run(cost, feed_dict={X: merged_views_original, Y: merged_views_original})\n",
        "          else:\n",
        "            session.run(optimizer, feed_dict={X: np.concatenate([merged_views_original,merged_views_original_missing],axis=0), Y: np.concatenate([merged_views_original,merged_views_original],axis=0)})\n",
        "            hidden_layer_output = session.run(L2, feed_dict={X: merged_views_original_missing})\n",
        "            error = session.run(cost, feed_dict={X: np.concatenate([merged_views_original,merged_views_original_missing],axis=0), Y: np.concatenate([merged_views_original,merged_views_original],axis=0)})\n",
        "            #loss2 =\n",
        "          if step % 1000 == 0:\n",
        "                #error = session.run(cost, feed_dict={X: merged_views_original_missing, Y: merged_views_original})\n",
        "                print(f\"Step: {step}, Error: {error}\")\n",
        "\n",
        "        reconstruction_error = session.run(cost, feed_dict={X: merged_views_original_missing, Y: merged_views_original})\n",
        "        print(\"Reconstruction Error on Data: \", reconstruction_error)\n",
        "\n",
        "        # Get hidden layer output\n",
        "        hidden_layer_output = session.run(L2, feed_dict={X: merged_views})\n",
        "\n",
        "        # Save the hidden layer output\n",
        "        np.savetxt(\"hidden_layer_output.txt\", hidden_layer_output, delimiter=\",\")\n",
        "\n",
        "    # Reload hidden layer output and print its shape\n",
        "    hidden_layer_output = np.loadtxt(\"hidden_layer_output.txt\", delimiter=\",\")\n",
        "    print(\"Hidden Layer Output Shape: \", hidden_layer_output.shape)\n",
        "\n",
        "\n",
        "\n",
        "    ######################################################################################################\n",
        "    # K-Means Clustering for Hidden Layer Output\n",
        "    import numpy as np\n",
        "    from sklearn.decomposition import PCA\n",
        "    pca = PCA(n_components=4)\n",
        "    hidden_layer_output1= np.concatenate((hidden_layer_output,merged_views),axis=1)\n",
        "    n_clusters = 3\n",
        "    kmeans_hidden = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "    #x1_pca = pca.fit_transform(hidden_layer_output)\n",
        "    kmeans_hidden.fit(hidden_layer_output)\n",
        "    labels_hidden = kmeans_hidden.labels_\n",
        "    # cluster_labels = kmeans.labels_\n",
        "    print(labels_hidden)\n",
        "    #######################################################################################################\n",
        "    # K-Means Clustering for Original Data\n",
        "    kmeans_original = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "    kmeans_original.fit(X_original[indices1, :])\n",
        "    labels_original = kmeans_original.labels_\n",
        "    print(labels_original)\n",
        "    y_true = y_true1 [indices1]\n",
        "    nmi_hidden = normalized_mutual_info_score(y_true, labels_hidden)\n",
        "    ari_hidden = adjusted_rand_score(y_true, labels_hidden)\n",
        "    # Calculate NMI and ARI for Original Data\n",
        "\n",
        "    print(\"Hidden Layer Output from Autoencoder:\")\n",
        "    print(f\"NMI: {nmi_hidden:.4f}\")\n",
        "    print(f\"ARI: {ari_hidden:.4f}\")\n",
        "    nmi_original = normalized_mutual_info_score(y_true, labels_original)\n",
        "    ari_original = adjusted_rand_score(y_true, labels_original)\n",
        "\n",
        "    print(f\"\\nOriginal Dataset:\")\n",
        "    print(f\"NMI: {nmi_original:.4f}\")\n",
        "    print(f\"ARI: {ari_original:.4f}\")\n",
        "\n",
        "    # Apply K-Means for each view\n",
        "    def apply_kmeans(view, k=3, n_init=10):  # Set n_init explicitly to suppress the warning\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=n_init)\n",
        "        clusters = kmeans.fit_predict(view)\n",
        "        return clusters\n",
        "\n",
        "    # Apply to different views\n",
        "    clusters_view1 = apply_kmeans(view1, k=3)\n",
        "    clusters_view2 = apply_kmeans(view2, k=3)\n",
        "    clusters_view3 = apply_kmeans(view3, k=3)\n",
        "    clusters_view4 = apply_kmeans(view4, k=3)\n",
        "    clusters_view5 = apply_kmeans(view5, k=3)\n",
        "    clusters_view6 = apply_kmeans(view6, k=3)\n",
        "    clusters_view7 = apply_kmeans(view7, k=3)\n",
        "    clusters_view8 = apply_kmeans(view8, k=3)\n",
        "    clusters_view9 = apply_kmeans(view9, k=3)\n",
        "    clusters_view10 = apply_kmeans(view10, k=3)\n",
        "\n",
        "\n",
        "    # Evaluate Clustering for each view\n",
        "    def evaluate_clustering(clusters, true_labels):\n",
        "        nmi = normalized_mutual_info_score(true_labels, clusters)\n",
        "        ari = adjusted_rand_score(true_labels, clusters)\n",
        "        return nmi, ari\n",
        "\n",
        "    nmi_view1, ari_view1 = evaluate_clustering(clusters_view1, y_true)\n",
        "    nmi_view2, ari_view2 = evaluate_clustering(clusters_view2, y_true)\n",
        "    nmi_view3, ari_view3 = evaluate_clustering(clusters_view3, y_true)\n",
        "    nmi_view4, ari_view4 = evaluate_clustering(clusters_view4, y_true)\n",
        "    nmi_view5, ari_view5 = evaluate_clustering(clusters_view5, y_true)\n",
        "    nmi_view6, ari_view6 = evaluate_clustering(clusters_view6, y_true)\n",
        "    nmi_view7, ari_view7 = evaluate_clustering(clusters_view7, y_true)\n",
        "    nmi_view8, ari_view8 = evaluate_clustering(clusters_view8, y_true)\n",
        "    nmi_view9, ari_view9 = evaluate_clustering(clusters_view9, y_true)\n",
        "    nmi_view10, ari_view10 = evaluate_clustering(clusters_view10, y_true)\n",
        "\n",
        "\n",
        "\n",
        "    print(f'View 1 => NMI={nmi_view1}, ARI={ari_view1}')\n",
        "    print(f'View 2 => NMI={nmi_view2}, ARI={ari_view2}')\n",
        "    print(f'View 3 => NMI={nmi_view3}, ARI={ari_view3}')\n",
        "    print(f'View 4 => NMI={nmi_view4}, ARI={ari_view4}')\n",
        "    print(f'View 5 => NMI={nmi_view5}, ARI={ari_view5}')\n",
        "    print(f'View 6 => NMI={nmi_view6}, ARI={ari_view6}')\n",
        "    print(f'View 7 => NMI={nmi_view7}, ARI={ari_view7}')\n",
        "    print(f'View 8 => NMI={nmi_view8}, ARI={ari_view8}')\n",
        "    print(f'View 9 => NMI={nmi_view9}, ARI={ari_view9}')\n",
        "    print(f'View 10 => NMI={nmi_view10}, ARI={ari_view10}')\n",
        "\n",
        "\n",
        "    # Calculate NMI and ARI for merged output\n",
        "    def calculate_nmi_ari(merged_output, true_labels):\n",
        "        labels_pred = KMeans(n_clusters=3, n_init=10, random_state=42).fit(merged_output).labels_\n",
        "        nmi = normalized_mutual_info_score(true_labels, labels_pred)\n",
        "        ari = adjusted_rand_score(true_labels, labels_pred)\n",
        "        return nmi, ari\n",
        "\n",
        "    nmi_merged, ari_merged = calculate_nmi_ari(merged_views, y_true)\n",
        "    print(f\"Merged View: NMI = {nmi_merged:.4f}, ARI = {ari_merged:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "G00LCxDOpknz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9xuiDM5ppB4"
      },
      "source": [
        "# **Iris Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "72HuF4IVpkfH"
      },
      "outputs": [],
      "source": [
        "# dataset(iris1,iris1_labels_original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0QYQDq6qeGy"
      },
      "source": [
        "iris new all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "R--JjEkEqEso"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "R20fBF-nqEnH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "zhBJlVKFqEjr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "iu0rcrRoqEaV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "t9kcQj3LqESB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "NjQH1Kf0pkUx",
        "outputId": "6c23cded-737e-4509-db37-e5ebfd1889d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75\n",
            "Merged Views Shape: (75, 40)\n",
            "Merged Views original Shape: (75, 40)\n",
            "Merged Views original Shape: (75, 40)\n",
            "Step: 0, Error: 0.07718420773744583\n",
            "Step: 1000, Error: 0.06963908672332764\n",
            "Step: 2000, Error: 0.0688195526599884\n",
            "Step: 3000, Error: 0.06797237694263458\n",
            "Step: 4000, Error: 0.06707862764596939\n",
            "Step: 5000, Error: 0.0662243664264679\n",
            "Step: 6000, Error: 0.06523149460554123\n",
            "Step: 7000, Error: 0.06414467096328735\n",
            "Step: 8000, Error: 0.0629483014345169\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-5b000ef6cc85>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_newall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miris1_labels_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-6ca3d615cc0c>\u001b[0m in \u001b[0;36mdataset_newall\u001b[0;34m(dataset, dataset_labels)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmerged_views_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmerged_views_original\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged_views_original\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerged_views_original_missing\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged_views_original\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerged_views_original\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mhidden_layer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmerged_views_original_missing\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged_views_original\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerged_views_original_missing\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged_views_original\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerged_views_original\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    972\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    973\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1215\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1216\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1394\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1395\u001b[0m                            run_metadata)\n\u001b[1;32m   1396\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1399\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1402\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1382\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1385\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1475\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1476\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1477\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1478\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                                             run_metadata)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "dataset_newall(iris1,iris1_labels_original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p29wg-9cqW4R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-yuU5BlQK2S"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDcJIum_qY8p"
      },
      "source": [
        "# **Monk1 Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPkVy-NoqYbk"
      },
      "outputs": [],
      "source": [
        "# dataset(monk1,monk1_labels_original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmuX3wF1rjs8"
      },
      "source": [
        "monk1 new all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8lrle1QriEB"
      },
      "outputs": [],
      "source": [
        "# dataset_newall(monk1,monk1_labels_original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNJnx_GsrpoV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g073fIALruYS"
      },
      "source": [
        "# **Monk2 Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kil6ISmmrs8t"
      },
      "outputs": [],
      "source": [
        "# dataset(monk2,monk2_labels_original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2GTdxvhr6xM"
      },
      "source": [
        "monk2 new all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLVCZX0Wr5tp"
      },
      "outputs": [],
      "source": [
        "# dataset_newall(monk2,monk2_labels_original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtF5suTysAPP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4afDKYYsFpn"
      },
      "source": [
        "# **Monk3 Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86Ujdag9sBbK"
      },
      "outputs": [],
      "source": [
        "# dataset(monk3,monk3_labels_original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7yKDLACsOD3"
      },
      "source": [
        "monk3 new all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyxAj7PpsM7e"
      },
      "outputs": [],
      "source": [
        "# dataset_newall(monk3,monk3_labels_original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV2utsHjsUbt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WYiGuCivztw"
      },
      "source": [
        "# **WPBC Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKskZmzSu8ku"
      },
      "outputs": [],
      "source": [
        "# dataset(WPBC,WPBC_labels_original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0RdRXBMv5AQ"
      },
      "source": [
        "WPBC new all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VEK5q0Dv77o",
        "outputId": "3e8cc2a8-0cb3-4058-b525-17f211f5d009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97\n",
            "Merged Views Shape: (97, 340)\n",
            "Merged Views original Shape: (97, 340)\n",
            "Merged Views original Shape: (97, 340)\n",
            "Step: 0, Error: 0.11091005802154541\n"
          ]
        }
      ],
      "source": [
        "dataset_newall(WPBC,WPBC_labels_original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ3iatHQwDRU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3WGCdvXwFsT"
      },
      "source": [
        "# **Wdbc Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtLBsDhTwD0d"
      },
      "outputs": [],
      "source": [
        "# dataset(Wdbc,Wdbc_labels_original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rzU3RpjwLjf"
      },
      "source": [
        "Wdbc new all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVBmLqDcwD3F",
        "outputId": "0b797be7-c4dc-446a-ad30-ddda2c3e5bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "285\n",
            "Merged Views Shape: (285, 300)\n",
            "Merged Views original Shape: (284, 300)\n",
            "Merged Views original Shape: (284, 300)\n",
            "Step: 0, Error: 0.12600049376487732\n",
            "Step: 1000, Error: 0.021628636866807938\n",
            "Step: 2000, Error: 0.02146437019109726\n",
            "Step: 3000, Error: 0.02130122110247612\n",
            "Step: 4000, Error: 0.0211391169577837\n",
            "Step: 5000, Error: 0.020997105166316032\n",
            "Step: 6000, Error: 0.020843932405114174\n",
            "Step: 7000, Error: 0.02069154381752014\n",
            "Step: 8000, Error: 0.020539885386824608\n",
            "Step: 9000, Error: 0.020388910546898842\n",
            "Reconstruction Error on Data:  0.020274617\n",
            "Hidden Layer Output Shape:  (285, 150000)\n",
            "[1 1 1 2 2 2 2 0 0 1 1 1 1 1 2 0 0 0 0 0 0 1 0 2 0 1 1 0 1 0 2 1 0 1 0 2 0\n",
            " 1 0 0 0 0 0 0 2 0 0 0 1 1 0 0 1 1 0 0 2 2 2 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 2 0 1 0 0 1 0 2 0 1 0 1 1 0 2 0 1 1 1 2 2 1 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 2 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 2 0 1 0 1 0 0 0 0 1 0 0 1 2 2 0 0 0 0 1 1 1 0 0 0 0 1 2 0 0\n",
            " 0 0 0 0 0 0 1 1 0 2 1 1 0 1 0 0 2 1 0 0 0 0 0 0 0 2 1 0 2 0 0 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            "[0 0 0 2 2 2 2 1 1 0 0 0 0 0 2 2 1 1 1 1 1 0 1 2 1 0 0 1 0 1 2 0 1 0 1 0 1\n",
            " 0 1 1 1 1 1 1 2 1 1 1 0 0 1 1 0 0 1 1 2 2 2 1 0 1 0 1 0 0 1 1 2 1 1 0 1 1\n",
            " 0 1 1 1 0 1 0 1 0 1 2 1 0 1 1 0 1 2 1 0 1 0 0 1 2 1 0 0 0 2 2 0 0 1 1 1 1\n",
            " 1 0 1 0 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0\n",
            " 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 2 1 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 1 1 1 2 1 0 1 0 1 1 1 1 0 1 1 0 2 2 1 1 1 1 0 0 0 1 1 1 1 0 2 1 1\n",
            " 1 1 1 1 1 1 0 0 1 2 0 0 1 0 1 1 2 0 1 1 1 1 1 1 1 2 0 2 2 1 1 1 0 0 0 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1]\n",
            "Hidden Layer Output from Autoencoder:\n",
            "NMI: 0.5963\n",
            "ARI: 0.6989\n",
            "\n",
            "Original Dataset:\n",
            "NMI: 0.5758\n",
            "ARI: 0.6917\n",
            "View 1 => NMI=0.5800136961401863, ARI=0.6993001855974947\n",
            "View 2 => NMI=0.5411540604527564, ARI=0.6730106186048718\n",
            "View 3 => NMI=0.5532105391231098, ARI=0.6845094105627383\n",
            "View 4 => NMI=0.5800136961401863, ARI=0.6993001855974947\n",
            "View 5 => NMI=0.5533053337730119, ARI=0.6429641781581544\n",
            "View 6 => NMI=0.5924215912027004, ARI=0.6951812368624117\n",
            "View 7 => NMI=0.5771774891283441, ARI=0.6950915764053542\n",
            "View 8 => NMI=0.5924215912027004, ARI=0.6951812368624117\n",
            "View 9 => NMI=0.5963071266720412, ARI=0.6989289462492312\n",
            "View 10 => NMI=0.5771774891283441, ARI=0.6950915764053542\n",
            "Merged View: NMI = 0.5904, ARI = 0.6913\n"
          ]
        }
      ],
      "source": [
        "dataset_newall(Wdbc,Wdbc_labels_original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yUSCdzCvwD6p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lqirrMbwah8"
      },
      "source": [
        "# **Balance Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tzwQal3TwZkY"
      },
      "outputs": [],
      "source": [
        "# dataset(balance,balance_labels_original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk4zQotQwtI2"
      },
      "source": [
        "balace new all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eBNQ7UB_whA-"
      },
      "outputs": [],
      "source": [
        "dataset_newall(balance,balance_labels_original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXLLniQawhEh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfZ699SPw2it"
      },
      "source": [
        "# **Cardio Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYeHiqUOwhHw"
      },
      "outputs": [],
      "source": [
        "# dataset(cardio,cardio_labels_original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TGQVIwdxMlS"
      },
      "source": [
        "cardio new all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNrzX6yow1l7"
      },
      "outputs": [],
      "source": [
        "dataset_newall(cardio,cardio_labels_original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hESQPi4ww1ok"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cGX0NXGwvYt"
      },
      "source": [
        "# **Pima Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jB7lyRYrw1sT"
      },
      "outputs": [],
      "source": [
        "dataset_newall(pima,pima_labels_original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbPcU06Hw3wX"
      },
      "source": [
        "# **Seeds Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi010s0uw-HR"
      },
      "outputs": [],
      "source": [
        "dataset_newall(seeds,seeds_labels_original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88bDZTo-xUhA"
      },
      "source": [
        "# **Glass Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7XGfH_ExaQ3"
      },
      "outputs": [],
      "source": [
        "dataset_newall(glass,glass_labels_original)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HCHOkawxeQz"
      },
      "source": [
        "# **Vehicle Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArXF5rtlxkAD"
      },
      "outputs": [],
      "source": [
        "dataset_newall(vehicle,vehicle_labels_original)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHKhugP6xqRr"
      },
      "source": [
        "# **Wine Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqelao-sxuC3"
      },
      "outputs": [],
      "source": [
        "dataset_newall(wine,wine_labels_original)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZg1TPKfxxia"
      },
      "source": [
        "# **pendigit Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ngyO9mmEx10z"
      },
      "outputs": [],
      "source": [
        "dataset_newall(pendigit,pendigit_labels_original)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myFMTkT94e-_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}